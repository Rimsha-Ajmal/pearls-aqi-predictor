{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJYXDZ8-yk4u"
      },
      "source": [
        "# Incremental Retrain RandomForest & Update Hopsworks Model Registry (Timestamp-based)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S704OsveylGW",
        "outputId": "fd4c9b7c-32ac-4208-928a-9b1f6454c97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hopsworks==4.2.*\n",
            "  Downloading hopsworks-4.2.9-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyhumps==1.6.1 (from hopsworks==4.2.*)\n",
            "  Downloading pyhumps-1.6.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (2.32.4)\n",
            "Collecting furl (from hopsworks==4.2.*)\n",
            "  Downloading furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Collecting boto3 (from hopsworks==4.2.*)\n",
            "  Downloading boto3-1.40.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pandas<2.2.0 (from hopsworks==4.2.*)\n",
            "  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pyjks (from hopsworks==4.2.*)\n",
            "  Downloading pyjks-20.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mock (from hopsworks==4.2.*)\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting avro==1.11.3 (from hopsworks==4.2.*)\n",
            "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (2.0.44)\n",
            "Collecting PyMySQL[rsa] (from hopsworks==4.2.*)\n",
            "  Downloading pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (5.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (2025.3.0)\n",
            "Collecting retrying (from hopsworks==4.2.*)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting hopsworks_aiomysql==0.2.1 (from hopsworks_aiomysql[sa]==0.2.1->hopsworks==4.2.*)\n",
            "  Downloading hopsworks_aiomysql-0.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting opensearch-py<=2.4.2,>=1.1.0 (from hopsworks==4.2.*)\n",
            "  Downloading opensearch_py-2.4.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (4.67.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (1.76.0)\n",
            "Collecting protobuf<5.0.0,>=4.25.4 (from hopsworks==4.2.*)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hopsworks==4.2.*) (25.0)\n",
            "Collecting sqlalchemy (from hopsworks==4.2.*)\n",
            "  Downloading SQLAlchemy-2.0.29-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.49.1->hopsworks==4.2.*) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.2.*) (2.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.2.*) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.2.*) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi>=2022.12.07 in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.2.*) (2025.10.5)\n",
            "Collecting numpy<2,>=1.26.0 (from pandas<2.2.0->hopsworks==4.2.*)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2.0->hopsworks==4.2.*) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2.0->hopsworks==4.2.*) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->hopsworks==4.2.*) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->hopsworks==4.2.*) (3.11)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy->hopsworks==4.2.*) (3.2.4)\n",
            "Collecting botocore<1.41.0,>=1.40.60 (from boto3->hopsworks==4.2.*)\n",
            "  Downloading botocore-1.40.60-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->hopsworks==4.2.*)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->hopsworks==4.2.*)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl->hopsworks==4.2.*)\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting javaobj-py3 (from pyjks->hopsworks==4.2.*)\n",
            "  Downloading javaobj_py3-0.4.4-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pyasn1>=0.3.5 in /usr/local/lib/python3.12/dist-packages (from pyjks->hopsworks==4.2.*) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.12/dist-packages (from pyjks->hopsworks==4.2.*) (0.4.2)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.12/dist-packages (from pyjks->hopsworks==4.2.*) (3.23.0)\n",
            "Collecting twofish (from pyjks->hopsworks==4.2.*)\n",
            "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from PyMySQL[rsa]->hopsworks==4.2.*) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->PyMySQL[rsa]->hopsworks==4.2.*) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->PyMySQL[rsa]->hopsworks==4.2.*) (2.23)\n",
            "Downloading hopsworks-4.2.9-py3-none-any.whl (664 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m665.0/665.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hopsworks_aiomysql-0.2.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
            "Downloading opensearch_py-2.4.2-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m258.6/258.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.60-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
            "Downloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading botocore-1.40.60-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading javaobj_py3-0.4.4-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: avro, twofish\n",
            "  Building wheel for avro (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123962 sha256=30f7f341af8a91e5ba64e806687035fe87a50caa0c3d249e4c609261167ff978\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/82/d3/8eb96fda033c7f1661086e2f8afb13f04817886d28b12f1e72\n",
            "  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twofish: filename=twofish-0.3.0-cp312-cp312-linux_x86_64.whl size=24319 sha256=64f755f17d93fbefbe2d2015b58ab41162883a79d16cff51a20475651af3bbaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/81/02/abf836d4acb19a3de48f6bfd738cb9bcb762978b835bca2faa\n",
            "Successfully built avro twofish\n",
            "Installing collected packages: twofish, pyhumps, javaobj-py3, sqlalchemy, retrying, PyMySQL, protobuf, orderedmultidict, numpy, mock, jmespath, avro, pyjks, pandas, opensearch-py, hopsworks_aiomysql, furl, botocore, s3transfer, boto3, hopsworks\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.44\n",
            "    Uninstalling SQLAlchemy-2.0.44:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.44\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.10.1 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMySQL-1.1.2 avro-1.11.3 boto3-1.40.60 botocore-1.40.60 furl-2.1.4 hopsworks-4.2.9 hopsworks_aiomysql-0.2.1 javaobj-py3-0.4.4 jmespath-1.0.1 mock-5.2.0 numpy-1.26.4 opensearch-py-2.4.2 orderedmultidict-1.0.1 pandas-2.1.4 protobuf-4.25.8 pyhumps-1.6.1 pyjks-20.0.0 retrying-1.4.2 s3transfer-0.14.0 sqlalchemy-2.0.29 twofish-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "244ef1945ce24d788d44bb27a22d7751"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install hopsworks==4.2.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import hopsworks\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from hsml.schema import Schema\n",
        "from hsml.model_schema import ModelSchema\n",
        "\n",
        "# ------------------------------\n",
        "# 1ï¸âƒ£ Connect to Hopsworks\n",
        "# ------------------------------\n",
        "print(\"ğŸ”— Connecting to Hopsworks...\")\n",
        "project = hopsworks.login(api_key_value=os.getenv(\"HOPSWORKS_API_KEY\"))\n",
        "fs = project.get_feature_store()\n",
        "mr = project.get_model_registry()\n",
        "print(\"âœ… Connected to project:\", project.name)\n",
        "\n",
        "# ------------------------------\n",
        "# 2ï¸âƒ£ Load feature group\n",
        "# ------------------------------\n",
        "fg = fs.get_feature_group(\"model_features\", version=None)\n",
        "df = fg.read()\n",
        "print(\"âœ… Loaded feature data. Shape:\", df.shape)\n",
        "\n",
        "# Ensure datetime column exists\n",
        "if \"datetime\" not in df.columns:\n",
        "    raise ValueError(\"âŒ 'datetime' column not found in feature group\")\n",
        "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
        "df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
        "\n",
        "# ------------------------------\n",
        "# 3ï¸âƒ£ Verify feature ingestion continuity\n",
        "# ------------------------------\n",
        "time_diffs = df[\"datetime\"].diff().dropna()\n",
        "avg_interval = time_diffs.mean().total_seconds() / 3600\n",
        "print(f\"â±ï¸ Average ingestion interval: {avg_interval:.2f} hours\")\n",
        "\n",
        "if avg_interval > 2:\n",
        "    print(\"âš ï¸ Warning: Feature ingestion not hourly (avg > 2h). Check pipeline.\")\n",
        "else:\n",
        "    print(\"âœ… Feature ingestion seems consistent (hourly or better).\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4ï¸âƒ£ Load last checkpoint\n",
        "# ------------------------------\n",
        "checkpoint_file = \"last_datetime.txt\"\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, \"r\") as f:\n",
        "        last_dt = pd.to_datetime(f.read().strip())\n",
        "    print(f\"â„¹ï¸ Last checkpoint found at: {last_dt}\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ No checkpoint found â€” starting fresh.\")\n",
        "    last_dt = pd.Timestamp.min\n",
        "\n",
        "# ------------------------------\n",
        "# 5ï¸âƒ£ Retraining frequency limiter\n",
        "# ------------------------------\n",
        "now_utc = datetime.now(timezone.utc)\n",
        "if (now_utc - last_dt).total_seconds() < 6 * 3600 and last_dt != pd.Timestamp.min:\n",
        "    print(\"â³ Skipping retraining: last update was within 6 hours.\")\n",
        "    sys.exit(0)  # âœ… clean exit (no warning)\n",
        "\n",
        "# ------------------------------\n",
        "# 6ï¸âƒ£ Load only new data\n",
        "# ------------------------------\n",
        "df_new = df[df[\"datetime\"] > last_dt].copy()\n",
        "if df_new.empty:\n",
        "    print(\"ğŸš« No new data to retrain. Exiting gracefully.\")\n",
        "    print(f\"â„¹ï¸ Last checkpoint remains at: {last_dt}\")\n",
        "    sys.exit(0)  # âœ… clean exit (no warning)\n",
        "\n",
        "print(f\"âœ… New data to train: {df_new.shape[0]} rows\")\n",
        "\n",
        "# ------------------------------\n",
        "# 7ï¸âƒ£ Convert scaled AQI â†’ numeric AQI\n",
        "# ------------------------------\n",
        "aqi_scale_map = {1: 50, 2: 100, 3: 150, 4: 200, 5: 300}\n",
        "aqi_cols = [c for c in df_new.columns if \"aqi\" in c.lower()]\n",
        "for col in aqi_cols:\n",
        "    df_new[col] = df_new[col].map(aqi_scale_map).fillna(df_new[col])\n",
        "print(f\"ğŸ”„ Converted scaled AQI columns to numeric AQI for {len(aqi_cols)} columns\")\n",
        "\n",
        "# ------------------------------\n",
        "# 8ï¸âƒ£ Prepare dataset\n",
        "# ------------------------------\n",
        "H = 72\n",
        "target_col = f\"aqi_t_plus_{H}\"\n",
        "\n",
        "if target_col not in df_new.columns:\n",
        "    raise ValueError(f\"âŒ Target column {target_col} not found.\")\n",
        "\n",
        "df_sup = df_new.dropna(subset=[target_col]).copy()\n",
        "non_feature_cols = [\"datetime\", \"timestamp\"]\n",
        "features = [c for c in df_sup.columns if c not in non_feature_cols + [target_col]]\n",
        "X = df_sup[features].copy()\n",
        "\n",
        "# Clean feature data\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X[X <= 0] = np.nan\n",
        "X = X.ffill().dropna()\n",
        "\n",
        "y = df_sup.loc[X.index, target_col].astype(float)\n",
        "\n",
        "split_frac = 0.8\n",
        "split_idx = int(len(X) * split_frac)\n",
        "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "print(f\"âœ… Data prepared | Train: {len(X_train)}, Val: {len(X_val)}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 9ï¸âƒ£ Train RandomForest\n",
        "# ------------------------------\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "def get_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mae, rmse, r2\n",
        "\n",
        "y_pred = rf.predict(X_val)\n",
        "mae, rmse, r2 = get_metrics(y_val, y_pred)\n",
        "print(f\"ğŸ“ˆ Validation -> MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# ğŸ”Ÿ Compare with existing model\n",
        "# ------------------------------\n",
        "model_name = f\"AQI_RandomForest_H{H}\"\n",
        "better = False\n",
        "\n",
        "try:\n",
        "    existing_model = mr.get_model(model_name, version=None)\n",
        "    old = existing_model.to_dict().get(\"metrics\", {})\n",
        "    old_mae, old_rmse, old_r2 = float(old.get(\"mae\", 999)), float(old.get(\"rmse\", 999)), float(old.get(\"r2\", -999))\n",
        "    print(f\"ğŸ“¦ Existing model v{existing_model.version}: MAE={old_mae:.4f}, RMSE={old_rmse:.4f}, RÂ²={old_r2:.4f}\")\n",
        "\n",
        "    improved = sum([\n",
        "        mae < old_mae * 0.98,\n",
        "        rmse < old_rmse * 0.98,\n",
        "        r2 > old_r2 * 1.01\n",
        "    ])\n",
        "\n",
        "    if improved >= 2 and r2 >= old_r2:\n",
        "        better = True\n",
        "        print(\"âœ… New model shows improvement â€” registering new version.\")\n",
        "    elif abs(mae - old_mae) < 0.001 and abs(rmse - old_rmse) < 0.001 and abs(r2 - old_r2) < 0.001:\n",
        "        print(\"â„¹ï¸ Model performance unchanged â€” skipping registration.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No significant improvement â€” keeping existing version.\")\n",
        "except Exception:\n",
        "    print(\"â„¹ï¸ No existing model found â€” registering first version.\")\n",
        "    better = True\n",
        "\n",
        "# ------------------------------\n",
        "# 1ï¸âƒ£1ï¸âƒ£ Register model if better\n",
        "# ------------------------------\n",
        "if better:\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    model_path = f\"models/{model_name}.pkl\"\n",
        "    joblib.dump(rf, model_path)\n",
        "\n",
        "    input_schema = Schema(X_train)\n",
        "    output_schema = Schema(y_train)\n",
        "    model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
        "\n",
        "    model = mr.sklearn.create_model(\n",
        "        name=model_name,\n",
        "        metrics={\"mae\": mae, \"rmse\": rmse, \"r2\": r2},\n",
        "        model_schema=model_schema,\n",
        "        description=f\"RandomForest AQI predictor ({H}-hour horizon)\"\n",
        "    )\n",
        "    model.save(model_path)\n",
        "    print(f\"âœ… Registered new model '{model_name}' with MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
        "else:\n",
        "    print(\"ğŸš« Model not registered â€” using existing version.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 1ï¸âƒ£2ï¸âƒ£ Update checkpoint\n",
        "# ------------------------------\n",
        "new_last_dt = df[\"datetime\"].max()\n",
        "with open(checkpoint_file, \"w\") as f:\n",
        "    f.write(str(new_last_dt))\n",
        "\n",
        "if new_last_dt > last_dt:\n",
        "    print(f\"ğŸ”– Updated checkpoint: {new_last_dt} âœ… (moved forward)\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Checkpoint not advanced: last={last_dt}, new={new_last_dt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "a5jRU-EL3bzP",
        "outputId": "23b52c05-40e6-47b9-d8ed-33c3c5d5f17a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Connecting to Hopsworks...\n",
            "Connection closed.\n",
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1251499\n",
            "âœ… Connected to project: pearls_aqi_predictor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VersionWarning: No version provided for getting feature group `model_features`, defaulting to `1`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.21s) \n",
            "âœ… Loaded feature data. Shape: (6776, 155)\n",
            "â±ï¸ Average ingestion interval: 1.06 hours\n",
            "âœ… Feature ingestion seems consistent (hourly or better).\n",
            "â„¹ï¸ Last checkpoint found at: 2025-10-26 11:17:04.411027+00:00\n",
            "ğŸš« No new data to retrain. Exiting gracefully.\n",
            "â„¹ï¸ Last checkpoint remains at: 2025-10-26 11:17:04.411027+00:00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}